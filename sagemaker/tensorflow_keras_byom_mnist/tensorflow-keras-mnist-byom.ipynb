{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker - Bring Your Own Model \n",
    "## TensorFlow + Keras 編\n",
    "\n",
    "ここでは TensorFlow と Keras を使ったサンプルコードを題材に、Amazon SageMaker 移行の方法を順を追って説明します。SageMaker Python SDK で TensorFlow を使うための説明は [SDK のドキュメント](https://sagemaker.readthedocs.io/en/stable/using_tf.html) にも多くの情報があります。\n",
    "\n",
    "注: \n",
    "ここで説明するのは Script モード という記法 (現時点では標準の書き方) で、FILE モード (入力データを Amazon S3 から学習時にファイルとしてコピーする方法) です。データサイズが大きくなった場合は、FILE Mode ではなく PIPE Mode をお使い頂いた方がスループットが向上します。\n",
    "また、ここでは以降手順の紹介のためトレーニングスクリプトは最小限の書き換えとしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. トレーニングスクリプトの書き換え\n",
    "\n",
    "### 書き換えが必要な理由\n",
    "Amazon SageMaker では、オブジェクトストレージ Amazon S3 をデータ保管に利用します。例えば、S3 上の学習データを指定すると、自動的に Amazon SageMaker の学習用インスタンスにデータがダウンロードされ、トレーニングスクリプトが実行されます。トレーニングスクリプトを実行した後に、指定したディレクトリにモデルを保存すると、自動的にモデルがS3にアップロードされます。\n",
    "\n",
    "トレーニングスクリプトを SageMaker に持ち込む場合は、以下の点を修正する必要があります。\n",
    "\n",
    "- 学習用インスタンスにダウンロードされた学習データのロード\n",
    "- 学習が完了したときのモデルの保存\n",
    "\n",
    "これらの修正は、トレーニングスクリプトを任意の環境に持ち込む際の修正と変わらないでしょう。例えば、自身のPCに持ち込む場合も、/home/user/data のようなディレクトリからデータを読み込んで、/home/user/model にモデルを保存したいと考えるかもしれません。同様のことを SageMaker で行う必要があります。\n",
    "\n",
    "### 書き換える前に保存先を決める\n",
    "このハンズオンでは、S3からダウンロードする学習データ・バリデーションデータと、S3にアップロードするモデルは、それぞれ以下のように学習用インスタンスに保存することにします。/opt/ml/input/data/train/といったパスに設定することは奇異に感じられるかもしれませんが、これらは環境変数から読み込んで使用することが可能なパスで、コーディングをシンプルにすることができます。1-1. 環境変数の取得で読み込み方法を説明します。\n",
    "\n",
    "#### 学習データ\n",
    "- 画像: /opt/ml/input/data/train/train.npz\n",
    "\n",
    "#### バリデーションデータ\n",
    "- 画像: /opt/ml/input/data/test/test.npz\n",
    "\n",
    "#### モデル\n",
    "- /opt/ml/model 以下にシンボルやパラメータを保存する\n",
    "\n",
    "### 書き換える箇所\n",
    "まず [サンプルのソースコード](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py) を以下のコマンドでダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/keras-team/keras/master/examples/mnist_cnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードされた mnist_cnn.py をファイルブラウザから見つけて開いて下さい (JupyterLab の場合は左右にファイルを並べると作業しやすいです)。あるいはお好きなエディターをお使い頂いても結構です。この mnist_cnn.py は、トレーニングスクリプト内で以下の関数を呼び出し、S3以外からデータをダウンロードしていますが、SageMaker では学習データを S3 からダウンロードして使用します。\n",
    "\n",
    "```(x_train, y_train), (x_test, y_test) = mnist.load_data()```\n",
    "\n",
    "学習データをダウンロードして、前述したように/opt/ml/input/data/train/といったパスから読み出して使います。書き換える点は主に3点です:\n",
    "\n",
    "1. 環境変数の取得<br>\n",
    "SageMaker では、学習データやモデルの保存先はデフォルトで指定されたパスがあり、これらを環境変数から読み込んで使用することが可能です。環境変数を読み込むことで、学習データの位置をトレーニングスクリプト内にハードコーディングする必要がありません。もちろんパスの変更は可能で、API経由で渡すこともできます。\n",
    "\n",
    "1. 学習データのロード<br>\n",
    "環境変数を取得して学習データの保存先がわかれば、その保存先から学習データをロードするようにコードを書き換えましょう。\n",
    "\n",
    "1. 学習済みモデルの保存形式と出力先の変更<br>\n",
    "SageMaker では Chainer の Estimator に対して deploy 関数を呼び出すことによってモデルをデプロイします。もとのtrain_mnist.pyでは、デプロイに十分な情報がありません。このサンプルでは npz 形式を使用するため、npz 形式でモデルが保存されるようにコードを追加します。その際、モデルの保存先を正しく指定する必要があります。学習が完了すると学習用インスタンスは削除されますので、保存先を指定のディレクトリに変更して、モデルがS3にアップロードされるようにします。\n",
    "\n",
    "### 1-0. 関数の定義\n",
    "ソースコードを SageMaker で使える状態にする前に、関数を定義して処理を関数の中に入れます。\n",
    "\n",
    "学習からモデルの保存までを train(args) 関数として定義します。ここでは次の手順で読み込む args でパラメータを受け取ります。S3 から取得したデータを読み込みトレーニングを行います。最後に、後の手順で定義する save(model, model_dir) 関数でモデルを保存します。\n",
    "\n",
    "def train(args) を定義し、mnist_cnn.py の 16行目から最後までを、def train(args)の中に入れます。\n",
    "\n",
    "```python\n",
    "def train(args):\n",
    "\n",
    "    batch_size = 128\n",
    "    num_classes = 10\n",
    "    epochs = 20\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    (中略)\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "```\n",
    "\n",
    "### 1-1. 環境変数の取得\n",
    "Amazon SageMaker では、トレーニングに用いるコードが実行時に Python スクリプトとして実行されます。その際、データ・モデルの入出力は こちら に記述があるよう SM_CHANNEL_XXXX や SM_MODEL_DIR という環境変数を参照する必要があります。そのため、argparse.ArgumentParser で渡された環境変数と、スクリプト実行時のハイパーパラメータを取得します。\n",
    "\n",
    "前の手順で作成した train() 関数の下に、以下の記述を追加します。\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--num-classes', type=int, default=10)\n",
    "    parser.add_argument('--epochs', type=int, default=12)\n",
    "    \n",
    "    # input data and model directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    \n",
    "    args, _ = parser.parse_known_args()\n",
    "    train(args)\n",
    "```\n",
    "\n",
    "mnist_cnn.py の 一番上に、以下の記述を追加します。\n",
    "\n",
    "```python\n",
    "import argparse, os\n",
    "```\n",
    "\n",
    "1-2. 学習データのロード\n",
    "\n",
    "前の手順で取得した環境変数の値を使って、train() 関数のはじめにある記述をを以下のように書き換えます。\n",
    "\n",
    "```python\n",
    "#     batch_size = 128\n",
    "#     num_classes = 10\n",
    "#     epochs = 12\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    epochs = args.epochs\n",
    "    num_classes = args.num_classes\n",
    "    train_dir = args.train\n",
    "\n",
    "    # load data \n",
    "    x_train = np.load(os.path.join(train_dir, 'train.npz'))['image']\n",
    "    y_train = np.load(os.path.join(train_dir, 'train.npz'))['label']\n",
    "    x_test = np.load(os.path.join(train_dir, 'test.npz'))['image']\n",
    "    y_test = np.load(os.path.join(train_dir, 'test.npz'))['label']\n",
    "    \n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "#     (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "```\n",
    "\n",
    "### 1-3. 学習済みモデルの保存形式と出力先の変更\n",
    "\n",
    "train() 関数の一番下に、以下の記述を追加します。\n",
    "```python\n",
    "save(model, args.model_dir)\n",
    "```\n",
    "\n",
    "mnist_cnn.py の 一番上に、以下の記述を追加します。\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "train() 関数の下に、以下の関数を追加します。\n",
    "\n",
    "```python\n",
    "def save(model, model_dir):\n",
    "    sess = K.get_session()\n",
    "    tf.saved_model.simple_save(\n",
    "        sess,\n",
    "        os.path.join(model_dir, 'model/1'),\n",
    "        inputs={'inputs': model.input},\n",
    "        outputs={t.name: t for t in model.outputs})\n",
    "```\n",
    "\n",
    "https://aws.amazon.com/jp/blogs/news/amazon-sagemaker-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  S3 location  |  環境変数  |  値  |\n",
    "| :---- | :---- | :----| \n",
    "|  s3://bucket_name/prefix/training  |  `SM_CHANNEL_TRAINING`  | `/opt/ml/input/data/training`  |\n",
    "|  s3://bucket_name/prefix/model.tar.gz  |  `SM_MODEL_DIR`  |  `/opt/ml/model`  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Notebook 上でのデータ準備\n",
    "\n",
    "トレーニングを始める前に、予め Amazon S3 にデータを準備しておく必要があります。この Notebook を使ってその作業をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok = True)\n",
    "\n",
    "np.savez('./data/train', image=x_train, label=y_train)\n",
    "np.savez('./data/test', image=x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "input_data = sagemaker_session.upload_data(path='./data', bucket=bucket_name, key_prefix='data/handson-byom-tensorflow-keras')\n",
    "\n",
    "print('Input data is uploaded to: {}'.format(input_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local Mode によるトレーニングとコードの検証\n",
    "トレーニングジョブを始める前に、Local Mode を使って、この Notebook インスタンス上でコンテナを立てコードをデバッグしましょう。\n",
    "\n",
    "`from sagemaker.tensorflow import TensorFlow` で読み込んだ SageMaker Python SDK の TensorFlow Estimator を作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "train_instance_type = \"local\"\n",
    "\n",
    "role = get_execution_role()\n",
    "estimator = TensorFlow(entry_point = \"./mnist_cnn.py\",\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       framework_version=\"1.12.0\",\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,\n",
    "                       hyperparameters={'batch-size': 64,\n",
    "                                        'num-classes': 10,\n",
    "                                        'epochs': 1})\n",
    "\n",
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`estimator.fit` によりトレーニングを開始しますが、ここで「チャネル」を指定すると、環境変数名 `SM_CHANNEL_XXXX` が決定されます。この例のように何も指定しない場合、デフォルトの `SM_CHANNEL_TRAINING` となります。\n",
    "\n",
    "`mnist.py` の中で書き換えが適切でない部分があったら、ここでエラーとなる場合があります。Local Mode ではクイックにデバッグができるので、正しく実行できるよう試行錯誤しましょう。\n",
    "\n",
    " `===== Job Complete =====`\n",
    "と表示されれば成功です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みモデルの確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $estimator.model_data ./\n",
    "!tar zxvf model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow `SavedModel` 形式で保存されたモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.Session() as sess:\n",
    "    tf.saved_model.load(sess, [tf.saved_model.tag_constants.SERVING], \"model/1/\")\n",
    "    \n",
    "    i = sess.graph.get_tensor_by_name('conv2d_1_input:0')\n",
    "    o = sess.graph.get_tensor_by_name('dense_2/Softmax:0')\n",
    "    \n",
    "    pred = sess.run(o, feed_dict={i:x_test[:10].reshape(-1, 28, 28, 1)})\n",
    "    print('pred:', np.argmax(pred, axis=1))\n",
    "    \n",
    "print('true:', y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. トレーニングジョブの発行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "role = get_execution_role()\n",
    "estimator = TensorFlow(entry_point = \"./mnist_cnn.py\",\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       framework_version=\"1.12.0\",\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,\n",
    "                       hyperparameters={'batch-size': 64,\n",
    "                                        'num-classes': 10,\n",
    "                                        'epochs': 4})\n",
    "\n",
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "```\n",
    "Billable seconds: <time>\n",
    "```\n",
    "と出力されればトレーニング終了です。これが実際にトレーニングインスタンスが課金される時間となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 推論エンドポイントのデプロイ\n",
    "\n",
    "`estimator.deploy` で、今トレーニングしたモデルを推論エンドポイントとしてデプロイすることができます。これには数分かかります。(`----!` と表示されればデプロイ完了です。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 5\n",
    "indices = random.sample(range(x_test.shape[0] - 1), num_samples)\n",
    "images, labels = x_test[indices]/255, y_test[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(images.reshape(-1, 28, 28, 1))['predictions']\n",
    "prediction = np.array(prediction)\n",
    "predicted_label = prediction.argmax(axis=1)\n",
    "print('The predicted labels are: {}'.format(predicted_label))\n",
    "print('true:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論エンドポイントは立てっぱなしにしていると時間で課金されるので、確認が終わったら忘れないうちに削除してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Spot training\n",
    "Amazon SageMaker では、[spot instance を使ったモデルの学習](https://aws.amazon.com/jp/blogs/news/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs/) が可能です。これにより、学習にかかるコストを最大90％抑えることが可能です。\n",
    "\n",
    "学習ジョブが終了すると、ログの最後に `Managed Spot Training savings: 64.4%` のような表示されます。この例の場合、学習にかかるコストが64.4% 削減できたことを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --quiet \"sagemaker>=1.37.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimator = TensorFlow(entry_point = \"./mnist_cnn.py\",\n",
    "                       role=role,\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       framework_version=\"1.12.0\",\n",
    "                       py_version='py3',\n",
    "                       script_mode=True,\n",
    "                       train_max_run = 5000,\n",
    "                       train_use_spot_instances = 'True',\n",
    "                       train_max_wait = 10000,\n",
    "                       hyperparameters={'batch-size': 64,\n",
    "                                        'num-classes': 10,\n",
    "                                        'epochs': 4})\n",
    "\n",
    "estimator.fit(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. まとめ\n",
    "\n",
    "TensorFlow と Keras を使った Amazon SageMaker への移行手順について紹介しました。普段お使いのモデルでも同様の手順で移行が可能ですのでぜひ試してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
