{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker - Bring Your Own Model \n",
    "## Chainer 編\n",
    "\n",
    "ここでは [Chainer](https://chainer.org/) のサンプルコードをAmazon SageMaker 上で実行するための移行手順について説明します。SageMaker Python SDK で Chainer を使うための説明は [SDK のドキュメント](https://sagemaker.readthedocs.io/en/stable/using_chainer.html) にも多くの情報があります。\n",
    "\n",
    "注: \n",
    "ここでは以降手順の紹介のためトレーニングスクリプトは最小限の書き換えとしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. トレーニングスクリプトの書き換え\n",
    "\n",
    "### 書き換えが必要な理由\n",
    "Amazon SageMaker では、オブジェクトストレージ Amazon S3 をデータ保管に利用します。例えば、S3 上の学習データを指定すると、自動的に  Amazon SageMaker の学習用インスタンスにデータがダウンロードされ、トレーニングスクリプトが実行されます。トレーニングスクリプトを実行した後に、指定したディレクトリにモデルを保存すると、自動的にモデルがS3にアップロードされます。\n",
    "\n",
    "トレーニングスクリプトを SageMaker に持ち込む場合は、以下の点を修正する必要があります。\n",
    "- 学習用インスタンスにダウンロードされた学習データのロード\n",
    "- 学習が完了したときのモデルの保存\n",
    "\n",
    "これらの修正は、トレーニングスクリプトを任意の環境に持ち込む際の修正と変わらないでしょう。例えば、自身のPCに持ち込む場合も、`/home/user/data` のようなディレクトリからデータを読み込んで、`/home/user/model` にモデルを保存したいと考えるかもしれません。同様のことを SageMaker で行う必要があります。\n",
    "\n",
    "### 書き換える前に保存先を決める\n",
    "\n",
    "このハンズオンでは、S3からダウンロードする学習データ・バリデーションデータと、S3にアップロードするモデルは、それぞれ以下のように学習用インスタンスに保存することにします。`/opt/ml/input/data/train/`といったパスに設定することは奇異に感じられるかもしれませんが、これらは環境変数から読み込んで使用することが可能なパスで、コーディングをシンプルにすることができます。[1-1. 環境変数の取得](#env)で読み込み方法を説明します。\n",
    "\n",
    "#### 学習データ\n",
    "- 画像: `/opt/ml/input/data/train/train.npz`\n",
    "\n",
    "#### バリデーションデータ\n",
    "- 画像: `/opt/ml/input/data/test/test.npz`\n",
    "\n",
    "#### モデル\n",
    "`/opt/ml/model` 以下にシンボルやパラメータを保存する\n",
    "\n",
    "### 書き換える箇所\n",
    "まず [サンプルのソースコード](https://raw.githubusercontent.com/chainer/chainer/v5/examples/mnist/train_mnist.py) を以下のコマンドでダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/chainer/chainer/v5/examples/mnist/train_mnist.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ダウンロードされた `train_mnist.py` をファイルブラウザから見つけて開いて下さい (JupyterLab の場合は左右にファイルを並べると作業しやすいです)。あるいはお好きなエディターをお使い頂いても結構です。この`train_mnist.py`は、トレーニングスクリプト内で以下の関数を呼び出し、S3以外からデータをダウンロードしていますが、SageMaker では学習データを S3 からダウンロードして使用します。\n",
    "\n",
    "```python\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "```\n",
    "\n",
    "学習データをダウンロードして、前述したように`/opt/ml/input/data/train/`といったパスから読み出して使います。書き換える点は主に3点です:\n",
    "1. 環境変数の取得  \n",
    "    SageMaker では、学習データやモデルの保存先はデフォルトで指定されたパスがあり、これらを環境変数から読み込んで使用することが可能です。環境変数を読み込むことで、学習データの位置をトレーニングスクリプト内にハードコーディングする必要がありません。もちろんパスの変更は可能で、API経由で渡すこともできます。\n",
    "    \n",
    "1. 学習データのロード  \n",
    "    環境変数を取得して学習データの保存先がわかれば、その保存先から学習データをロードするようにコードを書き換えましょう。\n",
    "\n",
    "1. 学習済みモデルの保存形式と出力先の変更  \n",
    "    SageMaker では Chainer の Estimator に対して deploy 関数を呼び出すことによってモデルをデプロイします。もとの`train_mnist.py`では、デプロイに十分な情報がありません。このサンプルでは npz 形式を使用するため、npz 形式でモデルが保存されるようにコードを追加します。その際、モデルの保存先を正しく指定する必要があります。学習が完了すると学習用インスタンスは削除されますので、保存先を指定のディレクトリに変更して、モデルがS3にアップロードされるようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"env\"></a>1-1. 環境変数の取得\n",
    "\n",
    "Amazon SageMaker では、トレーニングに用いるコードが実行時に Python スクリプトとして実行されます。その際、データ・モデルの入出力は [こちら](https://sagemaker.readthedocs.io/en/stable/using_chainer.html#prepare-a-chainer-training-script) に記述があるよう `SM_CHANNEL_XXXX` や `SM_MODEL_DIR` という環境変数を参照する必要があります。そのため、`argparse.ArgumentParser` で渡された環境変数と、スクリプト実行時のハイパーパラメータを取得します。\n",
    "\n",
    "![データのやりとり](./sagemaker-data-model.png)\n",
    "\n",
    "`SM_CHANNEL_TRAIN`, `SM_CHANNEL_TEST`, `SM_MODEL_DIR` の環境変数の値を取得するよう、以下の関数をトレーニングスクリプトの最初に追加します。この関数はデフォルトでこれらの環境変数を、`args.train`、`args.test`、`args.sm_model_dir` に格納します。また、 `SM_MODEL_DIR` は `model_dir` とは異なり、`args.model_dir` には常に S3 のパスが渡されます。\n",
    "\n",
    "```python\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "    parser.add_argument('--sm-model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--training-steps', type=int, default=20000)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args\n",
    "```\n",
    "\n",
    "\n",
    "これらの値は、create-training-jobのAPIを実行する際に (SageMaker Python SDK で estimator を呼び出す際に) 指定した hyperparameters の値に置き換えることができます。例えば、hyperparameters に `train`、`test`、`sm-model-dir`が指定されていれば、環境変数の値は hyperparameters の値で上書きされます。ここでは、学習のステップ数 `training-steps` はデフォルトで20000という値にしておいて、学習実行時に hyperparameters 経由で変更できるようにしておきましょう。そうすることで、デバッグ時に小さい training-steps で実行したりすることができます。\n",
    "\n",
    "\n",
    "`train_mnist.py`は、`if __name__ == \"__main__\":`から`main():`を実行します。最初に読み込むために、上記で定義した`parse_args()`を`main()`の冒頭に挿入して、`args`の中身を取り出します。このような記述になります。\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    train_dir = args.train\n",
    "    test_dir = args.test\n",
    "    sm_model_dir = args.sm_model_dir\n",
    "    training_steps = args.training_steps\n",
    "```\n",
    "\n",
    "これで学習データ・バリデーションデータの保存先を取得することができました。次にこれらのファイルを実際に読み込む処理を実装します。\n",
    "\n",
    "#### 確認\n",
    "これまでに、train_mnist.py に対して以下の変更を行いました。\n",
    "* parse_args() 関数の追加\n",
    "* main() 関数の始めに環境編集取得の記述を追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 学習データのロード\n",
    "\n",
    "train_mnist.py の先頭に、以下の記述を追加して必要はモジュールを import します。\n",
    "```python\n",
    "import numpy as np\n",
    "import os\n",
    "```\n",
    "\n",
    "もともとある以下の部分を、\n",
    "```python\n",
    "from chainer import training\n",
    "```\n",
    "以下のように追記します。\n",
    "```python\n",
    "from chainer import training, serializers\n",
    "```\n",
    "\n",
    "S3からダウンロードしたデータを読み込みコードを実装しましょう。環境変数から取得した `train_dir`や`test_dir` にデータを保存したディレクトリへのパスが保存され、それぞれ `/opt/ml/input/data/train`, `/opt/ml/input/data/test` となります。詳細は [ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html#your-algorithms-training-algo-running-container-trainingdata) をご覧下さい。\n",
    "\n",
    "今回は npz ファイルを読むようにコードを書き換えるので、以下のようなコードを main() 関数に追記します。パスが `train_dir`, `test_dir` に保存されていることをうまく利用しましょう。\n",
    "まずは、もともとある以下のコードをコメントアウトします。こちらは、mnistのデータを取得している部分です。\n",
    "```python\n",
    "# train, test = chainer.datasets.get_mnist()　#を先頭につけるとコメントアウトされます。\n",
    "```\n",
    "コメントアウトした部分の下に以下のコードを追記します。ここで、S3からダウンロードしたデータを読み込みます。\n",
    "```python\n",
    "    train_data = np.load(os.path.join(train_dir, 'train.npz'))['images']\n",
    "    train_labels = np.load(os.path.join(train_dir, 'train.npz'))['labels']\n",
    "\n",
    "    test_data = np.load(os.path.join(test_dir, 'test.npz'))['images']\n",
    "    test_labels = np.load(os.path.join(test_dir, 'test.npz'))['labels']\n",
    "\n",
    "    train = chainer.datasets.TupleDataset(train_data, train_labels)\n",
    "    test = chainer.datasets.TupleDataset(test_data, test_labels)\n",
    "```\n",
    "\n",
    "main() 関数の一番最後に、学習したモデルを保存するための以下の記述を追加します。\n",
    "```python\n",
    "serializers.save_npz(os.path.join(sm_model_dir, 'model.npz'), model)\n",
    "```\n",
    "\n",
    "\n",
    "#### 確認\n",
    "これまでに、train_mnist.py に以下の変更を行いました。\n",
    "* import の追加（numpy, os, serializers）\n",
    "* main() 関数の mnist のデータを取得する部分をコメントアウト\n",
    "* main() 関数に S3からダウンロードしたデータを読み込む記述を追加\n",
    "* main() 関数の最後にモデルを保存する記述を追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 推論用の関数を追加\n",
    "こちらが最後のソースコード変更作業です。<br>\n",
    "main() 関数の下に、以下の関数を追加します。こちらは、推論エンドポイントを立てる際に呼び出される関数です。\n",
    "```python\n",
    "def model_fn(model_dir):\n",
    "    model = L.Classifier(MLP(1000, 10))\n",
    "    serializers.load_npz(os.path.join(model_dir, 'model.npz'), model)\n",
    "    return model.predictor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Notebook 上でのデータ準備\n",
    "\n",
    "トレーニングスクリプトの書き換えは終了しました。　学習を始める前に、予め Amazon S3 にデータを準備しておく必要があります。この Notebook を使ってその作業をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習に利用する手書き数字データセットの MNIST を利用します。`chainer.datasets.get_mnist`を利用してデータセットをダウンロードし、学習用と検証用のデータをそれぞれ npz 形式で保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import shutil\n",
    "\n",
    "# Download MNIST dataset\n",
    "train, test = chainer.datasets.get_mnist()\n",
    "\n",
    "# Extract data and labels from dataset \n",
    "train_images = np.array([data[0] for data in train])\n",
    "train_labels = np.array([data[1] for data in train])\n",
    "test_images = np.array([data[0] for data in test])\n",
    "test_labels = np.array([data[1] for data in test])\n",
    "\n",
    "# Save the data and labels as .npz into local directories and upload them to S3\n",
    "\n",
    "os.makedirs('data/train')\n",
    "os.makedirs('data/test')\n",
    "\n",
    "np.savez('data/train/train.npz', images=train_images, labels=train_labels)\n",
    "np.savez('data/test/test.npz', images=test_images, labels=test_labels)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを Amazon S3 にアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker_session.upload_data(path='data/train', key_prefix='data/handson-chainer-mnist-byom/train')\n",
    "test_data = sagemaker_session.upload_data(path='data/test', key_prefix='data/handson-chainer-mnist-byom/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local Mode によるトレーニングとコードの検証\n",
    "トレーニングジョブを始める前に、Local Mode を使って、この Notebook インスタンス上でコンテナを実行してコードをデバッグしましょう。\n",
    "`from sagemaker.chainer.estimator import Chainer` で読み込んだ SageMaker Python SDK の Chainer Estimator を作ります。\n",
    "\n",
    "ここでは、学習に利用するインスタンス数 `train_instance_count` や  インスタンスタイプ `train_instance_type` を指定します。Local modeの場合は、`train_instance_type = \"local\"` と指定します。\n",
    "\n",
    "デバッグなので多くの学習ステップを回す必要はありません。`epoch` に 3 を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "from sagemaker.chainer.estimator import Chainer\n",
    "\n",
    "# instance_type = 'ml.m4.xlarge'\n",
    "instance_type = 'local'\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='train_mnist.py', role=role,\n",
    "                            train_instance_count=1, train_instance_type=instance_type,\n",
    "                            framework_version='5.0.0',\n",
    "                            hyperparameters={'epoch': 3, 'batchsize': 128})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`estimator.fit` によりトレーニングを開始しますが、ここで指定する「チャネル」によって、環境変数名 `SM_CHANNEL_XXXX` が決定されます。この例の場合、`'train', 'test'` を指定しているので、`SM_CHANNEL_TRAIN`, `SM_CHANNEL_TEST` となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer_estimator.fit({'train': train_data, 'test': test_data})\n",
    "\n",
    "# Keep the job name for checking training loss later \n",
    "training_job = chainer_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cnn_mnist.py` の中で書き換えを忘れた部分があったら、ここでエラーとなる場合があります。Local Mode ではクイックにデバッグができるので、正しく実行できるよう試行錯誤しましょう。\n",
    "\n",
    " `===== Job Complete =====`\n",
    "と表示されれば成功です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みモデルの確認\n",
    "\n",
    "Amazon S3 に保存されたモデルは普通にダウンロードして使うこともできます。保存先は `chainer_estimator.model_data` で確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS CLI を使ってノートブックインスタンス上にモデルをダウンロードして試しに推論します。\n",
    "\n",
    "このノートブックと同じディレクトリに tar.gz の形式でモデルをダウンロードして展開します。展開後のディレクトリ名は数字の羅列 (Unix time) になります。あとでモデルを読み込むため、正規表現を利用して、このフォルダ名を `model_dir` に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $chainer_estimator.model_data ./\n",
    "!tar -zxvf ./model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストデータセットからランダムに10枚選んでテストを行います。先ほど保存した`model_dir`からモデルをロードして、テストデータを入力します。ローカルモードでは学習を少ししか実行しなかったため、ほとんど正しい予測はできていないかもしれません。\n",
    "\n",
    "まずはロードするモデルを格納するためのモデルを定義し、モデルをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "class MLP(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_units, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            # the size of the inputs to each layer will be inferred\n",
    "            self.l1 = L.Linear(None, n_units)  # n_in -> n_units\n",
    "            self.l2 = L.Linear(None, n_units)  # n_units -> n_units\n",
    "            self.l3 = L.Linear(None, n_out)  # n_units -> n_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "    \n",
    "model = L.Classifier(MLP(1000, 10))\n",
    "chainer.serializers.load_npz('model.npz', model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、ロードした学習済みモデルを使って推論を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 10\n",
    "indices = random.sample(range(test_images.shape[0] - 1), num_samples)\n",
    "images, labels = test_images[indices], test_labels[indices]\n",
    "\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "with chainer.using_config('train', False):\n",
    "    prediction = model.predictor(images)\n",
    "    \n",
    "\n",
    "predicted_label = prediction.data.argmax(axis=1)\n",
    "print('The predicted labels are: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. トレーニングジョブの発行\n",
    "\n",
    "推論されていればコードのデバッグは完了です。次に、Amazon SageMaker のトレーニングジョブとしてトレーニングします。データ・モデルの入出力は変わらず S3 なので、`train_instance_type` に `ml.` で始まる SageMaker のインスタンスを指定するだけで実行できます。(リストは[こちら](https://aws.amazon.com/sagemaker/pricing/instance-types/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "chainer_estimator = Chainer(entry_point='train_mnist.py', role=role,\n",
    "                            train_instance_count=1, train_instance_type=instance_type,\n",
    "                            framework_version='5.0.0',\n",
    "                            hyperparameters={'epoch': 3, 'batchsize': 128})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chainer_estimator.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "```\n",
    "Billable seconds: <time>\n",
    "```\n",
    "と出力されればトレーニング終了です。これが実際にトレーニングインスタンスが課金される時間となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 推論エンドポイントのデプロイ\n",
    "\n",
    "`chainer_estimator.deploy` で、トレーニングしたモデルを推論エンドポイントとしてデプロイすることができます。これには数分かかります。(`----!` と表示されればデプロイ完了です。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = chainer_estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 10\n",
    "indices = random.sample(range(test_images.shape[0] - 1), num_samples)\n",
    "images, labels = test_images[indices], test_labels[indices]\n",
    "\n",
    "for i in range(num_samples):\n",
    "    plt.subplot(1,num_samples,i+1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap='gray')\n",
    "    plt.title(labels[i])\n",
    "    plt.axis('off')\n",
    "\n",
    "prediction = predictor.predict(images)\n",
    "predicted_label = prediction.argmax(axis=1)\n",
    "print('The predicted labels are: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-examples-jp/master/chainer_mnist/input.html\n",
    "from IPython.display import HTML\n",
    "HTML(open(\"input.html\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(data, dtype=np.float32)\n",
    "prediction = predictor.predict(image)\n",
    "predicted_label = prediction.argmax(axis=1)[0]\n",
    "print('What you wrote is: {}'.format(predicted_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推論エンドポイントは立てっぱなしにしているとお金がかかるので、確認が終わったら忘れないうちに削除してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainer を使った Amazon SageMaker への移行手順について紹介しました。普段お使いのモデルでも同様の手順で移行が可能ですのでぜひ試してみてください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
