{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker で PyTorch のコードを動かすサンプルノートブック\n",
    "このノートブックは、PyTorch のソースコードを Amazon SageMaker で動かすサンプルです。\n",
    "\n",
    "## このノートブックの使用上の注意\n",
    "- このノートブックでは，学習にp2.xlargeを使用するためやや料金がかかります。インスタンスタイプごとの料金に関しては[こちら](https://aws.amazon.com/jp/sagemaker/pricing/)をご参照ください。\n",
    "- このノートブックでは，約780MBの画像データをS3に保存しますので，不要になったら削除してください。\n",
    "\n",
    "## 下準備\n",
    "### Amazon SageMaker を使うための設定\n",
    "セッション情報、画像を保存するバケット名、ロールの取得を行います。バケット名のデフォルトは、sagemaker-[リージョン]-[アカウントID] です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-srgan'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 使用する画像のダウンロード\n",
    "このノートブックでは，[COCOデータセット](http://cocodataset.org/#download)を使用します。\n",
    "\n",
    "ダウンロードしたデータをtrainとvalidに分けて学習で使用します。\n",
    "\n",
    "The annotations in this dataset belong to the COCO Consortium and are licensed under a Creative Commons Attribution 4.0 License. The COCO Consortium does not own the copyright of the images. Use of the images must abide by the Flickr Terms of Use. The users of the images accept full responsibility for the use of the dataset, including but not limited to the use of any copies of copyrighted images that they may create from the dataset. Before you use this data for any other purpose than this example, you should understand the data license, described at http://cocodataset.org/#termsofuse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "# MSCOCO validation image files\n",
    "download('http://images.cocodataset.org/zips/val2017.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir data/valid\n",
    "!unzip -qo val2017.zip -d data\n",
    "!mv data/val2017 data/train\n",
    "!mv data/train/0000002* data/valid\n",
    "!mv data/train/0000005* data/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するPyTorchソースコードのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/examples.git\n",
    "! mv examples/super_resolution ./\n",
    "!rm -rf examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像をS3にアップロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "print(s3_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像のS3パスを設定\n",
    "学習時にこのパスを使用して学習用画像を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_data = 's3://' + bucket + '/' + prefix\n",
    "s3_train_data = s3_data + '/train'\n",
    "s3_valid_data = s3_data + '/valid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ソースコードの書き換え\n",
    "先ほどダウンロードしたソースコードは，Amazon SageMakerで使用できる状態にはなっていません。\n",
    "\n",
    "SageMakerを使って学習，推論させるためにソースコードを書き換える必要があります。\n",
    " \n",
    "Jupyterのファイルブラウザを表示し，`super_resolution/main.py`と`super_resolution/data.py`の2つのファイルを開きます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 不要箇所の削除（main.py）\n",
    "別の部分に記述を移動するため不要となる12から43行目を全て削除します。\n",
    "\n",
    "```python\n",
    "parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "parser.add_argument('--upscale_factor', type=int, required=True, help=\"super resolution upscale factor\")\n",
    "parser.add_argument('--batchSize', type=int, default=64, help='training batch size')\n",
    "parser.add_argument('--testBatchSize', type=int, default=10, help='testing batch size')\n",
    "（中略）\n",
    "print('===> Building model')\n",
    "model = Net(upscale_factor=opt.upscale_factor).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=opt.lr)\n",
    "```\n",
    "\n",
    "次に，main.pyの一番下のこの部分のコードを削除します。\n",
    "\n",
    "```python\n",
    "for epoch in range(1, opt.nEpochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "    checkpoint(epoch)\n",
    "```\n",
    "\n",
    "### 各種ライブラリとログ取得用の記述を追加（main.py）\n",
    "SageMaker，PyTorch，Pythonライブラリを使用するための記述と，ログをCloudWatchに出力するための記述を追加します。\n",
    "\n",
    "先ほど削除したコードがあったあたりに以下の記述を追加します。\n",
    "```python\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from sagemaker_containers.beta.framework import (content_types, encoders, env, modules, transformer,\n",
    "                                                 worker)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train()関数の変更（main.py）\n",
    "取得した環境変数の値を使って学習するようtrain()関数を変更します。\n",
    "```python\n",
    "def train(opt):\n",
    "    \n",
    "    torch.manual_seed(opt.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print('===> Loading datasets')\n",
    "    train_set = get_training_set(opt.upscale_factor, opt.train_dir)\n",
    "    test_set = get_test_set(opt.upscale_factor, opt.valid_dir)\n",
    "    training_data_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=opt.batch_size, shuffle=True)\n",
    "    testing_data_loader = DataLoader(dataset=test_set, num_workers=4, batch_size=opt.test_batch_size, shuffle=False)\n",
    "\n",
    "    print('===> Building model')\n",
    "    model = Net(upscale_factor=opt.upscale_factor).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=opt.lr)\n",
    "\n",
    "    num_epoch = opt.epochs\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for iteration, batch in enumerate(training_data_loader, 1):\n",
    "            input, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(input), target)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"===> Epoch[{}]({}/{}): Loss: {:.4f}\".format(epoch, iteration, len(training_data_loader), loss.item()))\n",
    "\n",
    "        print(\"===> Epoch {} Complete: Avg. Loss: {:.4f}\".format(epoch, epoch_loss / len(training_data_loader)))\n",
    "\n",
    "        test(testing_data_loader, model, criterion)\n",
    "        checkpoint(epoch, model, opt.model_dir)\n",
    "    save_model(model, opt.model_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論時に使用する関数とモデルを定義する関数の追加（main.py）\n",
    "モデルを定義する関数`predict_fn()`は必ず実装が必要です。モデルのネットワーク構成を定義し，重みデータをロードしてモデルを返します。\n",
    "\n",
    "`input_fn(), predict_fn(), output_fn()`を実装しない場合は，[デフォルトの処理](https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/default_inference_handler.py)が実行されます。`input_fn()`で推論データを受け取って，データをモデルに入力できる形に変換します。`predict_fn()`は`input_fn()`の出力を受け取ってそれを入力として推論を行い，その結果を返します。`output_fn()`は`predict_fn()`の出力を受け取り，推論リクエストへのレスポンスとして返します。\n",
    "\n",
    "今回は，受け取った入力データ（画像）を`input_fn()`でnumpyにデコードしたのちPILImageに変換し，`predict_fn()`で変換したPILImageを使って推論してその結果をnumpyに変換し，`output_fn()`で推論結果を返します。\n",
    "\n",
    "main.pyの一番下に以下のコードを追加します。\n",
    "\n",
    "```python\n",
    "def input_fn(input_data, content_type):\n",
    "    logger.info('input_fn---')\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    np_array = encoders.decode(input_data, content_type)\n",
    "    \n",
    "    image = Image.fromarray(np.uint8(np_array)).convert('YCbCr')\n",
    "\n",
    "    return image\n",
    "\n",
    "def predict_fn(data, model):\n",
    "    logger.info(\"input type: \" + str(data.mode))\n",
    "    y, cb, cr = data.split()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = ToTensor()(y).view(1, -1, y.size[1], y.size[0])\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        image = image.cuda()\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    input_data = image.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_data)\n",
    "        \n",
    "    out = out.cpu()\n",
    "    out_img_y = out[0].detach().numpy()\n",
    "    out_img_y *= 255.0\n",
    "    out_img_y = out_img_y.clip(0, 255)\n",
    "    out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')\n",
    "\n",
    "    out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n",
    "    out_img = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB')\n",
    "\n",
    "    output = np.asarray(out_img)\n",
    "\n",
    "    return output\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    logger.info('output_fn--')\n",
    "    logger.info(accept)\n",
    "    \n",
    "    logger.info('predict size: ' + str(np.shape(prediction)))\n",
    "\n",
    "    return worker.Response(response=encoders.encode(prediction, accept), mimetype=accept)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    logger.info('model_fn---')\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    UPSCALE_FACTOR = 2\n",
    "    model = Net(upscale_factor=UPSCALE_FACTOR).to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n",
    "            model.load_state_dict(torch.load(f))\n",
    "    else:\n",
    "        with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n",
    "            model.load_state_dict(torch.load(f, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    return model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test()関数を変更（main.py）\n",
    "train()関数から呼び出されるtest()関数を，パラメタを引数で与えて以下のように変更します。\n",
    "```python\n",
    "def test(testing_data_loader, model, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    avg_psnr = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in testing_data_loader:\n",
    "            input, target = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            prediction = model(input)\n",
    "            mse = criterion(prediction, target)\n",
    "            psnr = 10 * log10(1 / mse.item())\n",
    "            avg_psnr += psnr\n",
    "    psnr_res = avg_psnr / len(testing_data_loader)\n",
    "    \n",
    "    print(\"===> Avg. PSNR: {:.4f} dB\".format(psnr_res))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルを保存する関数を追加（main.py）\n",
    "学習したモデルを保存するための関数を追加します。\n",
    "main.pyの一番下に以下のコードを追加します。\n",
    "```python\n",
    "def save_model(model, model_dir):\n",
    "    print(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, 'model.pth')\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "```\n",
    "\n",
    "### checkpointを保存する関数を変更（main.py）\n",
    "train()関数から呼び出されるcheckpoint()関数を，パラメタを引数で与えて以下のように変更します。\n",
    "```python\n",
    "def checkpoint(epoch, model, model_dir):\n",
    "    model_out_path = \"{}/model_epoch_{}.pth\".format(model_dir, epoch)\n",
    "    torch.save(model, model_out_path)\n",
    "    print(\"Checkpoint saved to {}\".format(model_out_path))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 環境変数の取得（main.py）\n",
    "学習時に学習用コンテナに渡される環境変数とハイパーパラメタを取得します。\n",
    "main.pyの一番下に以下のコードを追加します。\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train Super Resolution Models')\n",
    "    # Data and model checkpoints directories\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for testing (default: 64)')\n",
    "    parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',\n",
    "                        help='SGD momentum (default: 0.5)')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=100, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--backend', type=str, default=None,\n",
    "                        help='backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)')\n",
    "    parser.add_argument('--upscale_factor', type=int, default=2,\n",
    "                        help='upscale factor (default: 2)')\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\n",
    "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--valid-dir', type=str, default=os.environ['SM_CHANNEL_VALID'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    \n",
    "    train(parser.parse_args())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.pyから呼ばれる関数の変更（data.py）\n",
    "引数を使って関数を呼び出すよう，get_training_set()関数と，get_test_set()関数を書き換えます。\n",
    "\n",
    "```python\n",
    "def get_training_set(upscale_factor, train_dir):\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(train_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))\n",
    "\n",
    "\n",
    "def get_test_set(upscale_factor, test_dir):\n",
    "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
    "\n",
    "    return DatasetFromFolder(test_dir,\n",
    "                             input_transform=input_transform(crop_size, upscale_factor),\n",
    "                             target_transform=target_transform(crop_size))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "画像の準備が終わったら、学習を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch の estimator を定義\n",
    "PyTorch のコードや、学習に使用するインスタンスタイプ、インスタンス数、ハイパーパラメタなどを設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='main.py',\n",
    "                    source_dir='./super_resolution',\n",
    "                    role=role,\n",
    "                    framework_version='1.1.0',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.p2.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 2,\n",
    "                        'backend': 'gloo',\n",
    "                        'upscale_factor': 2\n",
    "                    },\n",
    "                   metric_definitions=[\n",
    "                   {'Name': 'training:Loss', 'Regex': '===> Epoch .*? Complete: Avg. Loss: (.*?)$'},\n",
    "                    {'Name': 'validation:PSNR', 'Regex': '===> Avg. PSNR: (.*?) dB'}\n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習開始\n",
    "estimator に対して fit 関数を呼ぶことで学習が開始します。\n",
    "\n",
    "学習用インスタンスが立ち上がり、 PyTorch のコンテナとソースコードがインスタンスにロードされ、学習が開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'train': s3_train_data, 'valid': s3_valid_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## リアルタイム推論\n",
    "学習が完了したら，学習したモデルを使って推論を行います。\n",
    "\n",
    "ここでは，リアルタイム推論の手順を説明します。\n",
    "### 推論用エンドポイント起動\n",
    "estimator に対して deploy 関数を実行すると、推論用エンドポイントが立ち上がります。\n",
    "\n",
    "エンドポイントが立ち上がるまでに１０分程度かかることがあります。\n",
    "\n",
    "deploy を実行したのち、進捗を示すハイフンが表示されたのちビックリマークが表示されたらエンドポイントが立ち上がったことを示します。\n",
    "\n",
    "推論用エンドポイントが立ち上がると、train.py の model_fn でモデルが構成され、学習した重みがロードされます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論の実行\n",
    "エンドポイントに画像を送って、結果を取得します。今回使用するモデルは，入力画像を縦横2倍にした超解像画像を出力します。\n",
    "\n",
    "画像がエンドポイントに送られると train.py の input_fn が実行され、その結果が predict_fn（今回は定義なしなのでデフォルトの動作が実行される）に入力されて推論が実行され、推論結果が output_fn に渡されて numpy に変換されて返ってきます。\n",
    "\n",
    "試しに、検証に使用した画像を１枚エンドポイントに送ってその結果を画像として保存します。このノートブックと同じフォルダに画像が保存されます。\n",
    "\n",
    "ハンズオンではほとんど学習が進んでいないので結果の画像はいまひとつだと思いますが、学習を進めれば良い結果が得られます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "filename = 'data/valid/000000201676.jpg'\n",
    "filename_base = os.path.basename(filename)\n",
    "image = Image.open(filename)\n",
    "image = np.asarray(image)\n",
    "\n",
    "srimage = predictor.predict(image)\n",
    "pil_img = Image.fromarray(srimage)\n",
    "\n",
    "outfilename = 'out_srf_' + str(2) + '_' + filename_base\n",
    "pil_img.save(outfilename)\n",
    "print(outfilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "エンドポイントは起動している間課金され続けるので、不要になったら削除します。\n",
    "\n",
    "コンソールからも削除することが可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [参考] 過去に学習したモデルから推論用エンドポイントを立てる\n",
    "過去に学習したモデルを使用したい場合があります。その場合は、PyTorchModel を使用してモデルを定義して deploy 関数を呼びます。\n",
    "\n",
    "以下のセルの`<your bucket> <your folder>`の部分を実際のものに書き換えてから実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data='s3://<your bucket>/<your folder>/model.tar.gz', \n",
    "    role=role,\n",
    "    framework_version='1.1.0',\n",
    "    entry_point='train.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 不要なリソースの削除\n",
    "本ハンズオンを実施すると，ノートブックインスタンス，推論用エンドポイント，S3において課金が発生した状態となります。\n",
    "\n",
    "不要なリソースは削除してください。削除方法は[こちら](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/ex1-cleanup.html)をご参照ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考情報\n",
    "\n",
    "### 本サンプルで使用した SRGAN コード\n",
    "こちらのPyTorch Exampleのソースコードを使用させていただきました。\n",
    "- https://github.com/pytorch/examples/tree/master/super_resolution\n",
    "\n",
    "### Amazon SageMaker の使い方を学ぶのに便利なサンプルノートブック\n",
    "- 画像分類　https://github.com/ohbuchim/demo-notebooks/blob/master/image_classification/Image-classification-transfer.ipynb\n",
    " - 画像分類のビルトインアルゴリズムを使用して，学習，ハイパーパラメータチューニング，リアルタイム推論，バッチ推論を行うノートブックです\n",
    "- 公式サンプルノートブック GitHub　https://github.com/awslabs/amazon-sagemaker-examples\n",
    "\n",
    "### Amazon SageMaker PyTorch コンテナ\n",
    "- コンテナ　　https://github.com/aws/sagemaker-pytorch-container\n",
    "- Servingコンテナ　　https://github.com/aws/sagemaker-pytorch-serving-container/blob/master/src/sagemaker_pytorch_serving_container/\n",
    "\n",
    "### 分散学習に関する情報\n",
    "- https://aws.amazon.com/jp/blogs/news/amazon-sagemaker-now-supports-pytorch-and-tensorflow-1-8/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
