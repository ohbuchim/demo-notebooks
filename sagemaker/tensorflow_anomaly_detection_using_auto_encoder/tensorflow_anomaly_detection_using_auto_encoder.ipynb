{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using Tensorflow Auto Encoder\n",
    "\n",
    "## Download images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://<Bucket>/<Prefix>/' ./normal --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "### train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "size = 512\n",
    "\n",
    "dirname = './train/'\n",
    "filelist = np.array(list(map(lambda x: dirname + x, os.listdir(dirname))))\n",
    "num_all = len(filelist)\n",
    "num_test = int(num_all*0.2)\n",
    "\n",
    "id_all   = np.random.choice(num_all, num_all, replace=False)\n",
    "id_valid  = id_all[0:num_test]\n",
    "id_train = id_all[num_test:num_all]\n",
    "\n",
    "file_train = filelist[id_train]\n",
    "file_valid = filelist[id_valid]\n",
    "\n",
    "\n",
    "train_data = np.array([np.array(Image.open(i).resize((size, size))).astype('float32') / 255 for i in file_train])[:,:,:,:3]\n",
    "valid_data = np.array([np.array(Image.open(i).resize((size, size))).astype('float32') / 255 for i in file_valid])[:,:,:,:3]\n",
    "\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = './test/Anomaly/'\n",
    "filelist_anomaly = np.array(list(map(lambda x: dirname + x, os.listdir(dirname))))\n",
    "\n",
    "test_data_anomaly = np.array([np.array(Image.open(i).resize((size, size))).astype('float32') / 255 for i in filelist_anomaly])[:,:,:,:3]\n",
    "\n",
    "print(np.shape(test_data_anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = './test/Normal/'\n",
    "filelist_normal = np.array(list(map(lambda x: dirname + x, os.listdir(dirname))))\n",
    "\n",
    "test_data_normal = np.array([np.array(Image.open(i).resize((size, size))).astype('float32') / 255 for i in filelist_normal])[:,:,:,:3]\n",
    "\n",
    "print(np.shape(test_data_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./x_train', train_data)\n",
    "np.save('./x_valid', valid_data)\n",
    "np.save('./x_test_anomaly', test_data_anomaly)\n",
    "np.save('./x_test_normal', test_data_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "input_train = sagemaker_session.upload_data(path='x_train.npy', key_prefix='sagemaker/autoencoder-test')\n",
    "input_valid = sagemaker_session.upload_data(path='x_valid.npy', key_prefix='sagemaker/autoencoder-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "input_dir = os.path.dirname(input_train)\n",
    "input_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "ae_estimator = TensorFlow(entry_point='autoencoder_color.py',\n",
    "                             role=role,\n",
    "                             train_instance_count=1,\n",
    "                             train_instance_type='ml.p3.2xlarge',\n",
    "#                             train_instance_type='local',\n",
    "                             framework_version='2.1.0',\n",
    "                             py_version='py3',\n",
    "                             debugger_hook_config=False,\n",
    "                             hyperparameters={'epoch':1000, 'size':size, 'train_data_name':'x_train.npy', 'valid_data_name':'x_valid.npy'},\n",
    "                             distributions={'parameter_server': {'enabled': True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_estimator.fit( input_dir, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ae_estimator.deploy(initial_instance_count=1, instance_type='ml.p2.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "すでに起動済みのエンドポイントを使用する場合はこちらを実行する。情報が上書きされてしまうため、上記セルで新しいエンドポイントを起動した直後にこちらを実行しないこと。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import RealTimePredictor\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.predictor import numpy_deserializer, npy_serializer\n",
    "predictor = TensorFlowPredictor('<Endpoint name>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict\n",
    "\n",
    "## Anomaly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "decoded_imgs_anomaly = []\n",
    "shape = np.shape(test_data_anomaly)\n",
    "\n",
    "for i in range(n):\n",
    "    tmp = test_data_anomaly[i].reshape(1, shape[1], shape[2], shape[3])\n",
    "    predictions = predictor.predict(tmp)\n",
    "    decoded_imgs_anomaly.append(predictions['predictions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs_normal = []\n",
    "shape = np.shape(test_data_normal)\n",
    "\n",
    "for i in range(n):\n",
    "    tmp = test_data_normal[i].reshape(1, shape[1], shape[2], shape[3])\n",
    "    predictions = predictor.predict(tmp)\n",
    "    decoded_imgs_normal.append(predictions['predictions'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize results\n",
    "\n",
    "## Define functions\n",
    "\n",
    "結果表示用関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(image, output, threshold, filename):\n",
    "\n",
    "    output = np.array(output)*255\n",
    "    img = np.array(image)*255\n",
    "    \n",
    "    diff = np.abs(img-output).astype('uint8')\n",
    "    tmp = diff.sum(axis=2)\n",
    "    edge = detect_edge(img)\n",
    "    \n",
    "    \n",
    "    H = signal.convolve2d(tmp, np.ones((5,5)), mode='same')\n",
    "    \n",
    "    # 後処理としてエッジ部分を除外\n",
    "    H = H - edge*1000\n",
    "    H = np.where(H < 0, 0, H)\n",
    "    \n",
    "    x,y = np.where(H > threshold)\n",
    "    \n",
    "    fig, (ax0, ax1, ax2,ax3,ax4) = plt.subplots(ncols=5, figsize=(16, 4))\n",
    "    ax0.set_axis_off()\n",
    "    ax1.set_axis_off()\n",
    "    ax2.set_axis_off()\n",
    "    ax3.set_axis_off()\n",
    "    \n",
    "    ax0.set_title(filename[:10])\n",
    "    ax1.set_title('reconstructed image')\n",
    "    ax2.set_title('diff ')\n",
    "    ax3.set_title('mask')\n",
    "    ax4.set_title('anomalies: '+str(len(x)))\n",
    "    \n",
    "    ax0.imshow(img.astype(int), interpolation='nearest') \n",
    "    ax1.imshow(output.astype(int), interpolation='nearest')   \n",
    "    ax2.imshow((diff*3).astype(int), cmap=plt.cm.viridis, vmin=0, vmax=255, interpolation='nearest')  \n",
    "    ax3.imshow(edge.astype(int), interpolation='nearest', cmap='gray')\n",
    "    ax4.imshow(img.astype(int), interpolation='nearest')\n",
    "    \n",
    "    ax4.scatter(y,x,color='red',s=0.1) \n",
    "\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # 結果をノートブックに出力する場合\n",
    "#     plt.show()\n",
    "    \n",
    "    # 結果を画像として出力する場合\n",
    "    plt.savefig('res_'+filename, dpi=300)\n",
    "    \n",
    "    return len(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "後処理のエッジ抽出関数の定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def detect_edge(img):\n",
    "    minVal = 300\n",
    "    maxVal = 400\n",
    "    SobelSize = 10\n",
    "\n",
    "    img = cv2.cvtColor((np.array(img)).astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    edges = cv2.Canny(img,minVal,maxVal,SobelSize)\n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "    res = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel)\n",
    "    kernel = np.ones((20,20),np.uint8)\n",
    "    # closing = cv2.morphologyEx(res, cv2.MORPH_CLOSE, kernel)\n",
    "    res = cv2.dilate(res,kernel,iterations = 1)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict anomaly images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力画像と再構成画像の差分がいくつより大きければ異常部分とみなすかを決めるしきい値。値が大きいほど異常部分とみなしにくくなる。\n",
    "threshold = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_anomaly = []\n",
    "for i in range(n):\n",
    "    anomalies = plot(test_data_anomaly[i], decoded_imgs_anomaly[i], threshold, os.path.basename(filelist_anomaly[i]))\n",
    "    anomalies_anomaly.append(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict normal images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_normal = []\n",
    "for i in range(n):\n",
    "    anomalies = plot(test_data_normal[i], decoded_imgs_normal[i], threshold, os.path.basename(filelist_normal[i]))\n",
    "    anomalies_normal.append(anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the value of anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Anomaly images: ', anomalies_anomaly, np.mean(anomalies_anomaly))\n",
    "print('Normal images: ', anomalies_normal, np.mean(anomalies_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
